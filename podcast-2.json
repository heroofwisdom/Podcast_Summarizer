{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Inverse Reinforcement Learning Without RL with Gokul Swamy - #643", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone, welcome to another episode of the Twiml AI podcast. I'm your host, Sam Charrington. And today I'm joined by Gokul Swamy. Gokul is a PhD student at the Robotics Institute at Carnegie Mellon University. Wherever you're listening to today's show, make sure you're subscribed and be sure to like, rate and review the show. Gokul, welcome to the podcast. Yeah, thank you so much for having me. Hey, I'm looking forward to digging into our conversation. We'll be talking about your research into the fields of efficient interactive learning and making good decisions without observable boundaries, with a particular emphasis on a few of the papers that you are presenting at this year's ICML conference. Before we dig into that though, I'd love to have you share a little bit about your background and how you came to the field. Yeah, good question. So I just wrapped up my third year at CMU. I spent a bunch of time there working on different things that are about sort of algorithms for this field called imitation learning, which is broadly speaking about how you can try and learn to make good decisions from data. Before that, I spent a few years at Berkeley, where I was a master's and undergrad student, and there I was working on methods for human-robot interaction. Even before that, I grew up in San Diego, where I tried to just maximize the amount of time I spent on the beach. -. Sounds worthwhile. Tell us a little bit about your research interests and the focus of your work. Yeah. So I think over the last few years, what I've been really trying to think about is how we can try and learn to make sequence of decisions. Well, from observing data, some sort of expert demonstrated doing the same thing. So perhaps the most kind of intuitive example of this is something like self-driving cars. So, you know, we put some person in a car, we strap the car up with sensors, we get them to drive all around Pittsburgh, and then we want to learn a program that lets the car do the same thing. And I spent a lot of time thinking about what are the right sorts of algorithms for that problem. And I think there's sort of kind of two parts of that that I'm most interested in. The first is how we do this sort of efficiently. And I mean efficiently in various senses, both in terms of the amount of data you need to use, the amount of compute you need to use, stuff like that. The other thing I'm really interested in is if we're trying to get a car to do the same thing as a person, they don't have the same sort of observation space in some sense, right? They don't see exactly the same way, they don't have the exact same sensors. And at that point, there's all these sorts of things that show up that affect a relationship you care about that you don't really observe. So let's say, you know, you're trying to predict from like some variable x to some variable y, and there's some unobserved variable u that affects both. This is usually called an unobserved confounder. And, you know, here I'm trying to predict from, you know, some sort of observations, some sort of how much I want to the steering wheel, how much I want to press the gas pedal, stuff like that. And for example, your self-driving car might not be able to pick up on the fact that a hand gesture from somebody in the car across from them actually means something. And when you kind of have this sort of partial observability, I think it's a really interesting question of how do you still learn well? So I've been thinking about, both in the idealized setting, how do you do it efficiently, and in the more real-world setting, where you don't perhaps get access to the same pieces of information, how do you make decisions well there? Awesome, and how does that tie into the research that you're presenting at the conference this year? Yeah, so I think we have a few papers at the conference, which I'm very thankful for. And they sort of touch on different parts of those topics. So we have one paper at the main conference, which is really focused on the question of how do we do imitation learning efficiently. So that paper, I think, is really cool, because it's a paper where the theory is very elegant. It's not a very sort of complicated idea mathematically, but it really, really works in practice. So we're quite excited about that. Is this the inverse RL paper? Yeah, yeah. So I can maybe talk a little bit about that if that would be good. Yeah, let's dig into it. Sure. So just before you do, though, the title of the paper is Inverse Reinforcement Learning Without Reinforcement Learning, which is an intriguing title. learning without reinforcement learning, which is an intriguing title. Thank you. So broadly speaking, in sciences we have sort of like forward problems and inverse problems. The forward problem is usually, okay, I have some kind of objective function and I'm going to optimize it to get something. And the inverse problem is, okay, given behavior of the optimal thing, what was the thing I was trying to optimize in the first place? And in the sort of inverse reinforcement learning setting, right, the forward problem is reinforcement learning. So I give you a word function, I want you to find the optimal policy under this word function. The inverse problem is, okay, I gave you data from the optimal policy, and I want you to figure out what was the word function that was being optimized here. If you think about the driving example, I might want to try to extract a function that tells you, okay, how much does this person care about staying away from the cars that are in front of them? How much do they care about adhering to the speed limits, things like that. I think it's a very natural question to ask, well, can't I just write down a function that does that? It's actually really quite hard when you try to do it, because I natural question to ask, well, can't I just write down a function that does that? And it's actually really quite hard when you try to do it, right, because I can tell you that, OK, I definitely care about observing the speed limit. I care about staying away from people that are in front of me. But exactly how much I care about those two things relative to each other is very hard to write down. So I think it makes a lot of sense to try and learn from data. Yeah. Yeah. It sounds difficult enough to do manually when you're only thinking about one of those relationships. But when you're talking about a highly dimensional set of features, it sounds very, very difficult. For sure. Yeah. Yeah. And especially if you really want to have good human-like behavior in a variety of settings. Most of the people are just paying attention to one thing when they're driving. All these things are balancing in their head. So you really do need to try and actually learn from data how to do this. And so talk about the motivation behind trying to apply inverse RL without RL. Yeah, so and maybe was that a motivation or was that a result? So I think the motivation here is really like how we can make it inverse reinforcement learning more computationally efficient. So there's sort of like pros and cons, I think, to the sort of inverse reinforcement learning approach to things. I think the pro is basically that you don't suffer from something called compounding errors. So the simplest approach you could think of trying to do for imitation learning, right, is basically purely offline supervised learning approach. So what I do is I get a set of expert states, I get a set of expert actions, I just regress between them, and I just roll out my car. The issue is that, of course at some point, you're gonna make a mistake, and you're gonna end up in a situation in which you didn't have any data before. And then you're not gonna know what to do, and you're just gonna keep making mistakes over and over again. So if you kind of look at the early self-driving work in the late 80s, this is sort of what they were trying to do. And the cars drove a little bit, but as soon as you tried to do something hard with them, they didn't work. The benefit of the inverse reinforcement learning approaches is because you're actually rolling out the learner's policy, seeing how things go, you're able to see, OK, when I turn left, this is actually where I ended up. You're not just looking at states from the expert state distribution. So if you kind of think of it in this sort of set of sort of, maybe in the kind of distribution shift setting, basically what interaction gives us is it lets us kind of get samples from the test distribution because we can actually drive and see where we end up. So this is really nice because it lets you, you know, kind of see where you're going to end up, so you're not going to make mistakes you don't expect. But the challenge, of course, then, is that you have to repeatedly interact with the simulator over and over again. And you have to solve these hard reinforcement learning problems. So conceptually, the way inverse reinforcement learning works is basically it's almost like a GAN, but in the space of trajectories. So your generator here is like a policy that's kind of coupled with world models or dynamics, kind give you two directories. And your your discriminator here basically is saying, Okay, I want to look at the difference between expert and learner trajectories. And then you do a policy update that is okay, let me take this learned discriminator and use as a reward function and do reinforcement learning with that. So the issue then is that you need to repeatedly solve a hard reinforcement learning problem at every single step of this procedure. Yeah, so that means you're going to spend a huge number of samples, right? We already know that reinforcement learning is really, really hard to actually do in the real world on on problems, right? And if I'm asking you to do it over and over again, well, it's kind of difficult. So our question in this work was, okay, there's all these benefits, the inverse reinforcement learning approach, but there are these severe computational issues. So how can we try and actually speed this up quite a bit? And you arrived at an approach that does not use reinforcement learning, in fact? Yeah. So it's effectively an approach that allows us to like learning to make sequential a sequence of decisions, but without the sort of part that makes reinforcement learning hard, which is exploration, right? So maybe sort of a concrete example, I like here is something like, imagine what we want to do is sort of act optimally in some sort of problem that looks basically like going down the paths of a binary tree. And basically I tell you that, okay, the reward function this person cared about is zero everywhere except for one of the leaf nodes in this tree. And you know, my discriminator says, okay, I'm gonna pick this one leaf node to be one, everywhere else to be zero. Then my learner needs to explore the entire tree to figure out where that one node is that is non-zero. And that's a huge waste of time and compute and everything. The sort of insight we had in this work was that, well, if I know the states the person actually was in, I knew the person always went to the left in this tree, went to the leftmost node, I don't actually need to look at the rest of the tree, right? the person always went to the left in this tree, went to the leftmost node, I don't actually need to look at the rest of the tree, right? Because they never went there, that's probably the wrong thing to do. So I should be focusing my optimization just on the leftmost path. And if you kind of think about it that way, I think it's pretty reasonable to say that, okay, if I really just focus on optimizing on states that were, you know, related to what I saw the expert do, I should be able to cut out a lot of this sort of unnecessary exploration. And so did you cut out all exploration or did you cut out unnecessary exploration, as you said, by constraining the search space of the states that you're looking at? Yeah, that's a really good question. So if I cut out all exploration, that's very close to doing something that's like fully offline, right? And at that point, we wouldn't really have any robustness to compounding errors, right? Because we could just sort of end up in a place we didn't expect. But if what I do is I have enough exploration that I learned to recover my own mistakes, but not so much that I, you know, explore the entire space of the world to figure out how to recover my own mistakes, but not so much that I explore the entire space of the world to figure out this one thing. I can balance these two things. So I can be both computationally efficient and not end up in situations I don't expect and don't know how to recover from. Sure. You constrain this search space and then you use some alternate method that's not reinforcement learning to navigate through it. Yeah. So what we did was actually was actually I think a very simple idea, which was that we basically did something very close to reinforcement learning, but we just changed the start state distribution to be that of the policy you want to imitate. And because of that, right, I'm still doing sequential decision making, I'm still learning to recover from all these step mistakes, but I'm saying, hey, you don't need to start at the top of this tree and figure out how to get to the bottom of the tree every single time. I'm telling you, okay, you're only gonna be on the left part of the tree, just make sure you're good there, and then you're fine. So this is kind of how you can sort of constrain the search space, but still actually doing search or reinforcement learning. So I think it's really like still, it's sort of I think the best of both worlds in the sense that you are getting rid of the part of reinforcement learning that's challenging, which is exploration, while keeping the part of it that is helpful, which is actually learning to recover from your own mistakes. And so talk a little bit about how you kind of assessed and evaluated your results for the paper. Yeah, so we thus far only got to try out things in simulation. I'm really excited about trying this out on more real world problems soon. But what we did was we basically picked some of these sort of Majoko sort of open AI gym environments. And what we tried to do is basically see, you know, how well can we sort of imitate some expert policy. So we know we trained some policy via reinforcement learning. And we said, Okay, we want to learn another policy that does the same thing. How can we do this quickly. And what we did was we picked problems that are very challenging exploration problems. So imagine something like you're controlling like a four legged creature that walks through like a large maze, right? You don't go through every single path in this maze if you're doing it with traditional reinforcement learning. But if we basically tell you, hey, these are the set of waypoints through the maze that you only need to care about, it's a much easier optimization problem. And when we tried it out on this sort of problem, we saw rather remarkable improvements in terms of the amount of interaction with the environment you needed. And there's the kind of computational benefit of it, but I also would say there's I think there's a bit of a safety benefit in the sense that if your environment is actually something that is in the real world, you don't want your learner going around and doing crazy things all the time. You want to sort of keep them in a reasonable place. And in this example, were you worried about kind of extracting waypoints from expert data or did you assume that that was a kind of a downstream or upstream. Ask the perspective that's a good question i guess i would say it's in. Upstream task but it's one that wasn't too bad basically what we could do is basically say that ok you know. Every time separate a few times you know where was the expert at that time. Every time separate a few times you know where was the expert at that time. And then we could just grab that point of your waypoint we can start the learner from this waypoint and see where they go. And so in terms of did you have existing data sets that you so what we did is we picked one of these standard environments that a bunch of different methods have been tried on. These environments also come with a set of data. So we actually took some of the data sets from the offline RL literature, and we used that set of data. And then we implemented each of the methods ourselves. And do you see this, you've used the autonomous vehicle analogy several times in describing this work, do you see this method scaling up to that application that's a far way from the four-legged open AI gym type of environment? Well, I mean, if you have four wheels, but I think it's actually a very reasonable application. They're actually it's one I'm pretty excited about. Another one I'm super excited about is I recently learned that a lot of the routes in Google Maps are now calculated using inverse reinforcement learning. I think it's actually one of the world's most widely deployed machine learning systems now. And I think that this sort of techniques could be sort of techniques would be really, really useful there for just like, you know, drastically reducing the amount of compute need to do to compute routes and stuff like that. And I think also for self driving, I think it's the sort of thing where, you know, even to this day, I think a good chunk of the self driving industry, really, the sort of bread and butter of the of the way their autonomous systems work is using different sorts of reinforcement learning or using inverse reinforcement learning techniques. So I could see this being applicable to a really wide set of kind of real world systems. And it's also a set of, it's also an application that I think is very reasonable, in the sense that one of the sort of tricky parts about our method and one thing we're trying to address in future work, is that I need to have a sort of a simulator environment where I can just put the learner in some state to start them off. Perhaps for real-world robots, this is really challenging. If I'm trying to get the robot to do a backflip, I don't know how I reset the robot to be flipped halfway up in the air and then start it. But a lot of stuff in the self-driving space, training is done in simulation. So this is actually a very reasonable thing. Like, it shouldn't be hard at all to just basically just move the car to a different position in the simulator. So it's the sort of thing where I think actually self-driving is a perfect application of this sort of algorithm. I'm also very excited about the mapping application, because I think both of them are settings where this sort of thing should be able to help a lot. The sort of third application I'm really excited about is kind of in the space of large language models. So if you think about the sort of training of these models, right, there's a often a sort of fine tuning step at the end called our LHF or reinforcement learning from human feedback. And there, once again, we're using a very expensive reinforcement learning procedure. But we also have data of what we actually wanted, right? We know this was the data that was generated that people preferred. So it feels like we should be able to do something very similar there to basically speed up that search a lot. And there, like, you know, given how computer-tensive it is to train these models, I could expect this to provide, like, really strong computational benefits. So I guess what I'm trying to say here is that I think that there is a wide set of problems that kind of satisfy the assumptions required for this method to work out well, and I'm really excited about those applications. But I also think it's a very interesting question to think about for the problems where we can't do this really complicated agile robotics, what are sort of alternative approaches that could be useful there? Awesome. You've also got a couple of workshop papers at the conference. One is complementing a policy with a different observation space. Can you tell us a little bit about that one and what you're trying to do there? Yeah, of course. So I think this sort of touches on what I was talking a little bit about earlier, which is, you know, trying to make decisions without, to all the observed features. So the setup for this paper, I think, was pretty interesting. So let's say we have data of some doctor or set of doctors interacting with some patients. So we get data of what treatment they gave and what notes they perhaps took down. And we also see whether the patient got better or worse, things like that. And then what we want to do is we want to learn some sort of decision support system, some sort of computerized agent that's able to help them. But because this agent is a computer rather than a person, they're not going to be able to see the same set of things. They're not going to get the same set of observations. And the question is, OK, how do I actually figure out how to help here? And let me try to give you a concrete example of why this is actually not an easy thing to do. So let's say I have a doctor who is prescribing chemotherapy to patients. And let's say the sort of feature they observe is some test that tells you with a high probability whether this person has cancer or not. And let's say this doctor of feature they observe is some test that tells you with a high probability whether this person has cancer or not. And let's say this doctor is a really good doctor. So they basically only give you chemotherapy if you actually need it, otherwise they don't. And let's say, just for the sake of argument, our computerized agent here doesn't observe the result of this test. Well, what it's going to see is that every single time someone got chemotherapy, they got better. So it's then going to say, okay, the optimal thing for me to do is prescribe everyone chemotherapy because it only makes people get better. And then if you try to say, okay, and you try to trust the system a bit more, unless the vast majority of people had an underlying cancer condition, people are going to get worse as a result. So the question in this paper was, okay, how do we still learn how to do this well? We spent a bunch of time using some techniques from the sort of causal inference literature to try and basically kind of correctly estimate the sort of effect of an intervention, like giving someone a drug, and try to use that to really learn these sort of decisions of what systems that are able to actually help. And so is this an application of causal modeling approaches? Yeah, yeah, fully, yeah. Okay. And so talk a little bit of causal modeling approaches? Yeah, yeah, fully. Okay. And so talk a little bit about the specific approach you took. Yeah, yeah. So we sort of considered a couple different settings in the paper, and each required a sort of, I guess, kind of different approach or different set of assumptions. So I think maybe the easiest setting was basically, okay, at train time, but not at test time, I get to see both sets of observations. So for whatever reason, I also get to see what the doctor saw. But you know, at test time, I'm not going to get to see that the system doesn't get to get that. You can basically use this technique called the backdoor adjustment that was, you know, pioneered by Peter Pearl back in the day. And that sort of lets you effectively use an important sampling correction to fix incorrect estimates. So basically, you'd be able to sort of figure out that, hey, you know, it's this feature I didn't observe that actually was causing this positive effect. So I shouldn't like kind of over misattribute it to something else. I shouldn't like overestimate the value of the chemotherapy. Of course, this is somewhat unrealistic setting. So then we spent some time thinking about harder settings. So one setting is okay, you don't get to see what the doctor was looking at. But I do tell you, you know, what was their probability of taking this action, you know, so how likely would they actually to give this person chemotherapy. And then what you can do is you can again do a sort of important sampling technique to kind of fix things. The sort of hardest setting is when I don't give you either of those. I just give you the actions, and I give you a different set of observations. And if you think about this, this is a really hard problem, right? There's no reason to believe without assumptions you could actually solve this problem, right? Like it's like accessing your self-driving car to be able to stop, even though it doesn't see the stop sign. It's a really hard problem. So in this case, you're giving the model a set of actions that doctors took to, you know, treatment actions without showing the outcomes. So we're giving them the actions and the outcomes, but not what the doctor saw when they were choosing to give them that treatment. Not the observations. Yeah, yeah, exactly. Exactly. Yeah. If we don't give them the outcomes, yeah, yeah, exactly, exactly. And if we don't give them the outcomes, it's very, yeah, really hard. So, you know, it's like, the way I like to think about this, it's almost like, imagine if you and I were playing a game, and it's to predict the outcome of a coin flip. And I get to see the outcome of the coin flip. I'm definitely going to win this game. Right? So, you can't do this without any assumptions. So, in particular, there's this technique from the sort of econometrics literature called a proxy correction, which simply put, basically says that, even if there's something I don't observe that influences a relationship I care about, so long as I actually have proxies for this, that kind of vary in interesting ways, I'm able to use these to basically kind of filter out the effect of this thing I don't observe. So what we actually use was sort of a more modern version of those techniques to basically kind of correctly estimate the effect of actually giving a person a drug. I mean, it sounds like a lot of what you're trying to do here is to correct for kind of sampling imbalance in your data set. Is that a fair assessment of challenge? That's an interesting question. I guess it's, it's a sort of correcting for sampling balance. And it's a very interesting sampling imbalance, where it's one where the sampling was done based on something you don't observe. And of that if you make conclusions based on this data without accounting for that fact you can i try and crack conclusions it's definitely a sort of that sort of thing but it's a very precise sort of something about. Right so the generator function of the you know what's in your observation set and what's not is kind of out of scope and so you've got this whole other set of observations that you just don't have access to and you're trying to as efficiently as possible kind of. Identify ways to modify the actions you would take on your observations based on some knowledge of what the rest of the world is like yeah that's a great way of putting it exactly. what the rest of the world is like. Yeah, that's a great way of putting it. Yeah, exactly. And then the other workshop paper is one called Learning Shared Safety Constraints from Multitask Demonstrations. Talk a little bit about the setting there. Sure, yeah. So I think for a variety of tasks you would want some sort of agent to do, there's kind of a background set of shared safety constraints you might care about, right? So, you know, regardless of whether somebody is cleaning the table or, you know, making a sandwich in your kitchen, they shouldn't set the kitchen on fire. That would be nice. Yeah, yeah, I think I'd be pretty mad if someone did that. And I think the question is, you know, how we can try and learn these sorts of constraints from demonstrations. And if you think about it, this is really similar to what I was talking about earlier, where we're trying to learn rewards from demonstrations. And if you think about it, this is really similar to what I was talking about earlier, where we're trying to learn rewards from demonstrations. And I think what we're trying to do is use a very sort of similar flavor of approach, but here to try and learn these sort of like safety constraints. And the idea here is that you're, it's similar in a sense to what we talked about previously. You've got a whole set of observations. The observations are all focused on what to do, and you want to infer what not to do, but there's certainly a lot of things that aren't done in your observation set. That's a really interesting insight. Exactly. I think the way we tried to frame this problem is basically this. Let's say I told you what the task the person was trying to do was. You know the reward function. And I also give you what they actually did. So any suboptimal action they took then has to be because, oh, there was a safety constraint, right? Like the reason they didn't, if they were trying to get to the destination as fast as possible, right, if they didn't run through all the other cars, it must be because they don't want to hit the other cars. So you can use this sort of comparison between the optimal behavior under the reward function and the actual behavior you saw to try to extract what explains this difference. But if you over-apply that heuristic, then you also limit your ability to make the process more efficient, learn better ways to do it, things like that. Is that kind of the core limitation that you're fighting against? Yeah, good question. I think the core limitation we are fighting against is, well, this problem is a really kind of ill-posed problem in some sense. In the sense that, if for everything I don't see, I could just say there's a constraint that you can't do that. Right. But it's possible that there was just no need to do that, not that it was unsafe to do that. So the kind of key there was just no need to do that, not that it was unsafe to do that. So the kind of key thing we were focusing on this work is saying, okay, how do we fix that problem? And our insight was that, well, what we need is just a lot of multitask data. And you see people doing all these different things in the environment, make a sandwich, you see them clean the table, you see them wash the dishes, and in none of these things you see them set the kitchen on fire, you can probably safely assume you're not supposed to set the kitchen on fire. So it's sort of aggregating this data from a lot of different tasks, so you don't kind of learn this overly conservative constraint. In a sense, to oversimplify, perhaps, that seems like the obvious solution to the problem, right? If you're observing, or if your model's kind of taking these observations and trying to identify what's unsafe and anything it doesn't see, it's going to deem unsafe. We'll just make sure it sees a lot and a lot of different things. If we're talking about the NRL type of setting, we've previously talked about how expensive that can be. Do you also look at the efficiency aspects of it? Is there something about the way that you approach multitaskness that helps to deal with that? Yeah, that's a really good question. So we didn't really explore it in much in this particular paper, but you could actually use the algorithm we talked about in the first paper for just, you know, basically resetting the states from the expert demonstrations to basically, you know, try and solve this problem faster. So you could basically take out the reinforcement learning part of this paper and just stick in the algorithm we had earlier, and I think basically everything would go through, both in theory and in practice. So that's why I think I'm particularly excited about the techniques we talked about in the first paper, because I think of it as a hammer that can be used for a really wide set of sequential decision-making problems. And so what data sets did you use for this particular paper? Yeah, so for this particular paper, we actually wanted to make the problem really hard. So we took some of the standard offline RL benchmarks, and we just made them much harder. And we actually have a result that I'm really excited about in this paper, which is that we have this agent, four-legged ant running on this maze, and we're able to recover all the walls of the maze, the full structure of the maze, without the ant ever interacting with any of the walls. Just by looking at paths through the maze. That was really exciting to me, because I was like, this is really good news. I've seen people try to approach this problem before, but usually they're not able to solve anything beyond a linear or tabular problem. It was just here where we were able to do something, I was like, I could actually imagine using this. And I think the reason that's true for this method is it's built on this kind of strong theoretical and algorithmic foundation of techniques from the inverse reinforcement learning literature, which given they work reliably in the real world for things like self-driving cars and mapping, it's not super surprising they also worked well for this problem. So I think it's sort of, by building on a rather solid piece of bedrock, I think I'm really happy that the results worked as well as they did. And so besides applying that first paper that we talked about, lots of different areas, what are you most looking forward to in terms of your research agenda? That's a good question. I think I have a few different things that I'm still trying to figure out. So one of them is the sort of key assumption we're making in that first paper is that we have access to what people in the theory literature will call a generative model access to the environment, which basically means that I can just plop the learner in some random state and then see what they do from there, right? And I'm very interested in the question of, well, if we can't do that, which is true for certain problems, how do we still curtail unnecessary exploration? And I think that's like a very interesting theoretical question. I don't know if I have like a super good answer to it yet. And I think that's a very interesting theoretical question. I don't know if I have a super good answer to it yet, but I think basically, you could imagine something of the form that anytime you go pretty far outside of the state distribution of the expert, we just assume bad things happen. And the learner should learn, hey, I should probably not do that. So that's one thing I'm very excited about. Another thing I'm pretty excited about these days is these kind of space of large language models. I think they're a really interesting kind of domain because I think the core way you get them started is with a supervised training. And I just spent a bunch of time arguing about, basically, you should care about these better algorithms. You shouldn't just do supervised learning. But then LLMs, you just do supervised learning. But then LLMs, right, you just do supervised learning for at least getting them started, and it works great. And I think the sort of insight there is, well, if you have a huge amount of data, like a whole internet's worth, and you have a model with however many bajillion parameters, then perhaps you don't need to care as much about the right algorithms for these problems. But, as soon as you start to get to a problem where you don't have as much data, then I think you need to care more about how do you do things in an efficient manner. And I think the place you see that for LLMs is in the fine-tuning steps, so specifically the RLHF step. There, because you actually need to get a person to rate which of these completions was better, you can't get an infinite amount of data of this. And at that point, you have to be a lot more careful about algorithms. So I'm really excited about trying to do that problem more efficiently. The other problem there that I think is really technically interesting is, people always describe RLHF as an alignment procedure. And the question is, alignment with who? It's like with this pool of raters, all of whom might have different preferences and stuff like that. And there's a lot of literature in the economics and social choice theory spaces about basically how do we aggregate different preferences in a way that is reasonable. And I think there's a very interesting question there about how do we do that for reinforcement learning and human feedback. I spent some time thinking about that. The other problem, which I pulled it on the back burner, one of the settings in which I think you have this flavor of problem I really like, which is that you have repeated interaction, and you have kind of partial durability is problems in the recommendation space. So if I'm recommending you content, right, content, I recommend you changes your preferences in some way, right? Like if I tell you, I kind of introduce you to a new genre of music, well, then I'm probably going to, you know, want to listen to more music like that. But I don't actually ever observe your preferences, right? All I observe is that you clicked on this thing. So you're in this setting where it's very similar to the sort of settings I was thinking about earlier. And I think there's a really interesting space of trying to kind of adapt the algorithms I've talked about previously to these problems. And there's some preliminary work that's trying to get at this. I think there's some really beautiful work from the Spotify team on sort of treating recommendation as a sequential decision-making problem rather than just like a one-step problem and you see like really improved benefits. Actually, I think podcast recommendations in Spotify are now done using reinforcement learning. And I think that some of the techniques I've been working on might be really useful for dealing with the fact that, hey, we don't actually observe everything that's going on here. And we need to be a little bit more careful about the way we make decisions. Nice. Awesome. Well, thanks so much for joining us to share a bit about your ICML papers and some of your research. Yeah. Thank you so much for having me. Awesome. Thanks, Coco. All right, everyone, that's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimmel.ai.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening, and catch you next time. you"}, "podcast_summary": "In this podcast episode, Sam Charrington interviews Gokul Swamy, a PhD student at Carnegie Mellon University. Gokul discusses his research in the fields of efficient interactive learning and making good decisions without observable boundaries. They focus on a few of the papers that Gokul is presenting at this year's ICML conference.\n\nThe first paper they discuss is about inverse reinforcement learning without reinforcement learning. They explain that inverse reinforcement learning is the task of inferring the reward function from expert demonstrations, while reinforcement learning is the task of finding an optimal policy based on a given reward function. Gokul and his team propose a method that efficiently solves the inverse reinforcement learning problem without the need for reinforcement learning. They introduce a technique that constrains the search space by focusing on the states that are related to what the expert demonstrated. This approach reduces unnecessary exploration and removes the need for solving hard reinforcement learning problems at each step.\n\nThe second paper they discuss is about learning shared safety constraints from multitask demonstrations. Gokul explains that in many tasks, there are shared safety constraints that need to be learned from demonstrations. The challenge is that the model does not observe what is considered unsafe, so it tends to deem everything it doesn't see as unsafe. To address this, Gokul and his team propose a method that aggregates data from multiple tasks to learn these safety constraints. This allows the model to avoid overly conservative constraints and learn more efficient and effective ways to operate.\n\nGokul also mentions two other areas of interest in his research agenda. One is the challenge of dealing with problems where there is limited access to the environment, making exploration difficult. The other is improving efficiency in reinforcement learning from human feedback, particularly in the context of large language models.\n\nOverall, Gokul's research aims to improve the efficiency, safety, and decision-making capabilities of various AI systems, including self-driving cars, language models, and recommendation systems.", "podcast_guest": {"name": "Gokul Swamy", "summary": ""}, "podcast_highlights": "- Efficiently learning to make decisions from data is crucial in fields like self-driving cars and human-robot interaction.\n- Inverse reinforcement learning without reinforcement learning can speed up the process of learning optimal policies.\n- Learning shared safety constraints from multitask demonstrations can help AI agents understand what actions are safe in various situations.\n- The approach in the first paper can be applied to a wide range of problems, including autonomous vehicles and large language models.\n- Efficiently solving recommendation problems requires adapting sequential decision-making algorithms to the partial observability of user preferences."}